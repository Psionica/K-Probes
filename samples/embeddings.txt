Let's talk about semantic embeddings.

What field is this part of?
Computational semantics or natural language processing, depending on who you ask.

What is this a consequence of?
They're the result of exploiting word co-occurence information derived from large corpora.

How would you explain this to a teenager?
Imagine you're planning on travelling to a foreign country. The catch is, there's no map available! You see, it's quite an obscure country, perfect for a curious teen like you. The only thing available is a list of past itineraries - lists of cities which other travellers visited before, in order. The interesting thing is that you can use those itineraries to sketch a map! Cities visited one after another should be near each other. You could try to come up with a map which matches the distance relations, and you'll get a pretty accurate map you can use on your journey. In essence, language is like a set of itineraries, called sentences. However, instead of cities, it's got words! I just took you along a journey through the country of language, travelling from one word to the next. It turns out that if you try to use the same reasoning here, you can actually pinpoint words on a map, based solely on a lot of text. Not a map of cities, mountains, and rivers, but a map of meaning. Semantic embeddings are the "geographical" coordinates of words in this country of language.

What issues can this help with?
They can help machines extract meaning from otherwise plain text.

What other concepts are related to this?
Text mining, natural language processing, vector semantics, lexical semantics.

How can this be a source for inspiration?
If words can be represented as points in a space, what else could be represented this way? The effectiveness of semantic embeddings gets you thinking about what other things can be expressed as points in a space.

Why is this relevant?
It's the most popular approach to extracting meaning from text. It drives most, if not all, of modern natural language processing.

How do the components of this relate to each other?
Each type is associated with a vector. All vectors are part of the same space. There are relations between any two types, represented through the difference between their vectors.              

What has to be in place before this can be applied?
A lot of text for obtaining the embeddings. A broader understanding of the properties of semantic embeddings in academia and industry.

What can this lead to?
Machines which can reason about relatable concepts. Tools for thought, as well.